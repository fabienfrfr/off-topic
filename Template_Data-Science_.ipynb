{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide du DataScientist\n",
    "\n",
    "Enoncer l'introduction du problème, les données à disposition (brièvement), les contraintes (temps, puissance de calcul et mémoire de stockage) et l'objectif général (Collecte de donnée, analyse statistique exploratoire, contruction d'un modèle prédictif, etc.). Amélioration : ajouter \"NLP\" et Traitement d'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import os\n",
    "import pandas as pd, numpy as np\n",
    "import pylab as plt, seaborn as sns\n",
    "import geopandas as gpd, networkx as nx\n",
    "import osmnx as ox # openstreetmap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pyvis import network as net\n",
    "\n",
    "# exploration\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing as skpp\n",
    "from sklearn import imputeute as skimp\n",
    "from sklearn import model_selection as skms\n",
    "from sklearn import metrics as skm\n",
    "from sklearn import feature_selection as skfs\n",
    "\n",
    "from imblearn.over_sampling import RandomUnderSampler, SMOTE\n",
    "\n",
    "sklearn.cluster\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# save\n",
    "import joblib\n",
    "%store -r plot_var\n",
    "\n",
    "# param\n",
    "pd.set_option('display.max_row', 200)\n",
    "pd.set_option('display.max_column', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelque principe en informatique theorique pour comprendre comment fonctionne les programmes et les possibles optimisations :\n",
    "\n",
    "- Comparaison asymptotique BigO (Complexité temps/espace : iteratif vers recurcif)\n",
    "- Tri de liste (stack, queue, algo, etc.) et parcours de graphes.\n",
    "- Nombre,  theorie des automates / lamda calcul et parallelisme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Estimate complexity\n",
    "\n",
    "### Sort & graph\n",
    "\n",
    "### Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Collection\n",
    "\n",
    "Lorsque les données proviennes d'une base sous format fichier (Kaggle, INSEE, Yahoo Finance, France Data, etc.), indiquer le lien sous forme de liste à puce: \n",
    "- Contenu du fichier (type csv, excell) : [file_name/interested_sheet](https://www.link.fr/)\n",
    "- etc.\n",
    "\n",
    "Lorsque les données sont dans une base SQL, HDFS ou Spark, ou à extraire d'un contenu Web (Scraping), il convient de préparer les donnée sous un format adapté à l'analyse de donnée (moteur, conversion, etc.). Dans le cas du Web scraping, il est préférable de construire la base de donnée à partir d'un code python à part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SQL\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:myPassword@localhost/database_name')\n",
    "\n",
    "#### Distributed computing\n",
    "# Spark\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "#### Web scraping\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "html_address = \"https://fr.wikipedia.org/wiki/link\"\n",
    "r = requests.get(html_address)\n",
    "soup = bs(r.content, 'html.parser')\n",
    "\n",
    "# extract content\n",
    "contents = soup.prettify()\n",
    "table = soup.find_all(class_ = 'parent_tag')\n",
    "for t in table :\n",
    "    list_ = table.select('child_tag')\n",
    "    for l in list_ :\n",
    "        row_ = {}\n",
    "        row = l.find_all('subtag')\n",
    "        for i,r in row.items() :\n",
    "            sub_content = r.getText(\"\", strip=True)\n",
    "            # clean & store\n",
    "            row_[i] = sub_content.replace('\\n', ' ').replace('\\t', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-a. Data knowledge\n",
    "\n",
    "Noter les connaissances à disposition sur les données en question, se baser principalement sur Wikipédia. Dans certain cas, utiliser les rapports à disposition sur les donnée (exemple : INSEE, article, etc.).\n",
    "\n",
    "Anoncer les ordres de grandeurs qu'on imagine (nombre d'habitant, etc.) de sa propre culture générale (avant d'approfondir)\n",
    "\n",
    "Faire des remarques sur les données, par exemple sur la taille des echantillons, les notions de proportion, etc. L'idée est de mettre en avant la possibilité d'avoir des variables cachées dans les données pouvant nous tromper sur les mesures de corrélation future. Le cas le plus courant correspond au **paradoxe de Simpson**, un phénomène observé lorsque la tendance de plusieurs groupes s'inverse lorsque les groupes sont combinés :\n",
    "\n",
    "$$f<g, sup(f) < inf(g), \\exists (P,Q), $\\mathbb{E}_P (f) > $\\mathbb{E}_Q (g)$$\n",
    "\n",
    "Avec $\\mathbb{E}[X]=\\frac {\\sum_{i=1}^{n} x_i P_i}{\\sum_{i=1}^{n} P_i}$. Enfin, il est toujours possible d'ajouter des variables supplémentaires provenant d'autre base de donnée. Par exemple, nous pouvons ajouter des données géographiques d'openstreetmap [openstreetmap](https://www.data.gouv.fr/fr/datasets/decoupage-administratif-communal-francais-issu-d-openstreetmap/) ou de graphe (réseau, chaine de Markov, HashTable, etc.) si le problème s'y porte bien, cela peut au moins faciliter la visualisation des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-b. Data overview\n",
    "\n",
    "**Spreadsheet vizualization :**\n",
    "\n",
    "Visualiser les données sur un tableurs (Excel, Calc, etc.) avant de se lancer dans une quelquonque analyse. Identifier en premiers une variable permettant de joindre les fichiers entres eux, elle est principalement qualitative (object : string) ou encore à valeur entiere (int). Pour chacun des fichiers (s'il y en a), noter :\n",
    "\n",
    "- La liste des variables : Leurs types, la notion d'echelle, leurs dépendance et enfin en quoi l'ensemble corresponds (categorie).\n",
    "- La liste des variables regroupable en nouvelles variables (à partir d'un seuil, etc.).\n",
    "- etc.\n",
    "\n",
    "***Objectifs :*** Enoncer les sous-problemes à partir de ce que l'on vient d'apprendre sur les données (Comment ... ?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-c. Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv\n",
    "data_csv = pd.read_csv(os.getcwd() + '/file.csv')\n",
    "\n",
    "## excel\n",
    "xls_file = pd.ExcelFile('path.xls')\n",
    "data_xls = pd.read_excel(xls_file, sheet_name=['name'], skiprows=list(range(16)))\n",
    "\n",
    "## geographic\n",
    "data_geo = gpd.read_file(\"path.shp\")\n",
    "\n",
    "## sql\n",
    "data_sql = pd.read_sql('SELECT * FROM table LIMIT 3', engine)\n",
    "\n",
    "## Spark\n",
    "data_spk = sc.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy data for easy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data_csv.copy(), data_xls.copy(), data_geo.copy(), data_sql.copy(), data_spk.copy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Showing basic informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistique descriptive\n",
    "for df in data :\n",
    "    display(df.describe())\n",
    "\n",
    "## Résultat Naïf\n",
    "df[['columns1','columns2']].sum() / df['columns3'].sum()\n",
    "\n",
    "## Tableau croisé dynamique\n",
    "df.groupby(['columns1','columns2']).agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Pour chacun des éléments décrire ce que l'on mesure (résultats). Enoncer si la taille des echantillons sont différents, les ecarts moyennes globale/locale, s'il y a besoin de normaliser les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-d. Data pre-analysis\n",
    "\n",
    "Preparer les données à une analyse exploratoire (Correlation, Distribution) et inférentielle (Test d'hypothese et Intervalle de confiance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simplify dataframe\n",
    "\n",
    "Enlever les colonnes inutiles (superficial label) et standardiser et formater la nomenclature de jointure (merge). Aussi, enlever les variables sans variances $\\mathbb{V}(X) = \\mathbb{E}\\left[(X-\\mathbb{E}(X))^2\\right] = \\frac{1}{n}\\sum_{i=1}^n \\left(x_i - \\overline{x}\\right)^2$ (moyenne des carrés des écarts à la moyenne), car cela implique que la variables est constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all unusual object columns\n",
    "df.drop(columns=columns[:index+1], inplace=True)\n",
    "\n",
    "# standardize (here geographic exemple by modulo str digit like \"{:02d}\".format(values))\n",
    "merger_series  = df[column1].str.zfill(2) + df[column2].astype(str).str.zfill(3)\n",
    "df.insert(0, 'merge_column' , merger_series)\n",
    "\n",
    "# rename columns\n",
    "df.rename(columns={'old_name_column':'merge_column'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create variable\n",
    "\n",
    "Dans certain cas et suivant les connaissance du probleme, il est possible de combiner des colonnes en une seule, cela permet de reduire l'information. Ces nouvelles variables peuvent aussi bien quantitative (ex : valeurs mediane de plusieurs colonne) ou qualitative (nom de la meilleur colonne parmit plusieurs colonnes). Aussi, il est possible que des lignes soient vides, creer une variables indiquant laquelle est vide ou non, permet d'ajouter une nouvelle information pour notre analyse. Enfin, lorsqu'il y a des données continues manquantes, mais que l'on a des données qui permetrait d'interpoler (Exemple : 2 datasets à deux moments différents), nous pouvons réaliser un ajustement linéaire, attention toutefois à regrouper des catégories au préalable si necessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create best columns name\n",
    "df[\"best_columns\"] = df[['columnA','columnB']].idxmax(1).astype(\"category\")\n",
    "\n",
    "## create empty variable indicator\n",
    "df[\"empty\"] = (df.isna().sum(axis=1) < thresh).replace({True: 'empty', False: 'full'})\n",
    "\n",
    "## find median values of columns (error here)\n",
    "cumsum = df[['c1','c2','c3']].apply(lambda x : np.cumsum(x), axis=1)\n",
    "df[\"median_columns\"] = df[['c1','c2','c3']].med(1)\n",
    "\n",
    "## combine 2 colomns\n",
    "arr = df.values ; new_arr = arr[:,1::2]+arr[:,2::2] # 1st subcategory\n",
    "df_ = pd.DataFrame(arr, columns=['catA_'+str(int(i/2)) if i%2==0 else 'catB_'+str(int(i/2)) \n",
    "                                 for i in range(arr.shape[0]/2)]) # 2nd subcategory %2\n",
    "## interpolate by mean\n",
    "arr_bis = df_bis.values ; new_arr = (arr + arr_bis)/2\n",
    "df_ = pd.DataFrame(new_arr, columns=df.columns)\n",
    "\n",
    "## Measurment of qualitative new variable (exemple)\n",
    "df[\"best_columns\"].value_counts() / df[\"best_columns\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Lorsqu'on creer de nouvelle variable, il est interessant de mesurer directement ici la statistique de moyenne pour les valeurs qualitatives. Pour les autres, il est necessaire de voir plus en detail dans les parties suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalized data\n",
    "\n",
    "Lorsque les colonnes sont connecté entre elle par une colonne, il est preferable de normaliser, cela permet de conserver les informations relatives, tout en gardant la colonne d'echelle. Ici, ce n'est pas à confondre avec la standardisation des données. Ici on remplace les anciennes valeurs, mais le mieux est de creer de nouvelle données dans le cas où il y a un facteurs de taille \"critique\". On distingue plusieurs méthodes détaillé ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lorsqu'on a que quelque colonne\n",
    "to_norm_col = ['c1','c2','c3']\n",
    "df[to_norm_col] = df[to_norm_col].div(df[\"Scale\"], axis=0)\n",
    "\n",
    "## Lorsqu'on a beaucoup de colonne à partir d'un indice (ici 2)\n",
    "df.iloc[:,2:] = (df.iloc[:,2:]).div(data[1][\"Scale\"], axis=0)\n",
    "\n",
    "## Lorsqu'il n'existe pas de colonne d'echelle et mélangé avec colonne qualitative\n",
    "to_norm_col = df.select_dtypes('float').columns\n",
    "df[to_norm_col] = df[to_norm_col].div(df[to_norm_col].sum(axis=1), axis=0)\n",
    "\n",
    "## mesurment of scaling\n",
    "df[norm_col].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Ici, c'est souvant notre derniere mesure des effets de proportion. Ici nous pouvons justements comparer la formulation du paradoxe de Simpson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permet de générer de l'intuition sur les données avant de faire une analyse approfondit. Dans le cas de données géographique, il est interessant de mesurer aussi bien les valeurs à plusieurs echelle (commune, departement, region, pays et global). Dans le cas de donnés de graphes, une visualisation des interactions permets d'avoir des idée. Aussi, il faut visualiser les nouvelles variables que l'on a creer si possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## geographic representation local scale\n",
    "fig,ax = plt.subplots(figsize=(10, 10))\n",
    "geodf[geodf.CODE.str[:2] != \"97\"].plot(ax=ax, categorical=True, column = \"best_columns\", legend=True)\n",
    "\n",
    "## geographic representation up scale\n",
    "dep = geodf.dissolve(by='dep', aggfunc='sum') # attention si déjà normalisé\n",
    "dep[geodf.dep != \"97\"].plot(ax=ax, categorical=True, column = \"best_columns\", legend=True)\n",
    "\"\"\"dep.to_file(output_path)\"\"\"\n",
    "\n",
    "## network graph (social net, transport, Markov chain)\n",
    "G = nx.from_pandas_edgelist(df, 'Start', 'End')\n",
    "g = net.Network(notebook=True)\n",
    "nxg = nt.from_nx(nx_graph)\n",
    "g.from_nx(nxg); g.show(\"title.html\")\n",
    "\n",
    "## Boxplot de certaine colonne (new variable essentially)\n",
    "sns.catplot(data=df[['c1','c2','c3']],  kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Ces outils et ces observations seront importantes en cas de visualisation du modele pour comparer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-e. Data compilation\n",
    "\n",
    "Fusionner les dataframes par la méthode d'intersection (inner : $ A \\cap B = \\{ x:(x \\in A) \\wedge (x \\in B) \\} $ ). Ce qui permet d'avoir la taille optimale des données ayant les meme valuers de colonne de jointure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataf_list :\n",
    "    data = pd.merge(data, d, how='inner', on='merge_column') # automatics also with time series\n",
    "# head or tail\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas image :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas Texte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Explorer le jeu de donnée assemblé pour trouver les variables d'interet. Le but ici est de tester des hypothèses (correlation, multimodalité, etc.) et de savoir quel type de modèle on va utiliser (regression ou classification). Idéalement, la regression est utilisé dans le cas où l'on cherche une \"fonction continu\" qui va suivre notre nuage de points, alors que la classification est utilisé lorsqu'on a des groupes de nuages de points que l'on veut \"classer\". On copie toujours le jeu de donnée compilé pour ne pas à refaire l'importation (+ probleme de pointer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_explore = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble des concepts vu ici sont lié au notion d'esperance et de convergence des probabilités. Pour cela, on utilise l'Inégalité de Tchebychev pour majorer un variables aléatoires à une constante :\n",
    "\n",
    "$$ \\mathbb{P} \\left ( \\left | X \\right | \\geq \\alpha \\right ) \\leq \\frac{1}{\\alpha^p} \\mathbb{E}(\\left | X \\right |^p) $$ \n",
    "\n",
    "Pour p = 1 (Markov), cela veut dire que plus l'esperance de X est petite, moins il est probable que X prenne de grandes valeurs. Pour p=2 (Bieneymé), on mesure la disperstion autour de l'esperance.\n",
    "\n",
    "On démontre cette inégalité par la fonction indicatrice $ \\mathbf{1}_A(x) := \\left\\{\\begin{matrix} 1 & x \\in A\\\\ 0 & x \\notin A \\end{matrix}\\right.$ car elle rempli au maximum l'espace probabilisé en question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-a. Data definition\n",
    "\n",
    "**Variable target :** Correspond à la variable de sortie, la variable que l'on veut predire (on ne sait pas encore), si c'est une valeurs continu, on se tournera plutot vers un probleme de regression, si c'est une catégorie, un probleme de classification. Lister les possibilités ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_explore.shape) # row and columns\n",
    "df_explore.dtypes.value_counts().plot.pie() # variable type\n",
    "print((df_explore.isna().sum()/df_explore.shape[0]).sort_values(ascending=True)) # NaN proportion\n",
    "# for time series : see resample per date, rolling/ewn, FFT analysis (+possible filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici énoncé les possibilité d'avoir des contraintes pour l'analyse/modélisation. Décrire successivement :\n",
    "\n",
    "- Le nombre de variable : si trop, overfitting par exemple.\n",
    "- Le nombre d'échantillon : sous echantillonnage (critere de Nyquist pour systeme temporel, intervalle de confiance pour procédé aléatoire).\n",
    "- Les différents types de variable : N floats (feature) / N objects (label) / N ints\n",
    "- La proportions d'éléments vide par variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-b. Data relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation des correlations entre variables, ainsi que la distribution de chaque variables suivant les targets possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Correlation des données (Regression) :\n",
    "\n",
    "La mesure de corrélation entre variable est un outil/notion qui permet de mesurer la dépendance **linéaire** entre 2 variables aléatoire quantitative. Lorsque les corrélations sont comprise entre $[-0.4, 0.4]$, les varaibles ne sont pas corrélé, entre $[-0.8, -0.4[ U ]0.4, 0.8[$ faiblement (anti-)corrélé et jusqu'a -1,1, tres (anti-)corrélé. Attention, deux variables tres corrélé, ne veut pas dire relation de causalité/correspondance. Partant d'un echantillon, un estimateur (biaisé) du coefficient de corrélation est donné par :\n",
    "\n",
    "$$ r = \\frac {\\mathbb{E}[(X - \\mathbb{E} (X))(Y - \\mathbb{E} (Y)) ]}{\\sqrt{ V_X}  \\sqrt{ V_Y}} = \\frac {\\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y) }{\\sigma_X  \\sigma_Y} = \\frac {\\mathrm{Cov}(X,Y) }{\\sigma_X  \\sigma_Y}$$\n",
    "\n",
    "Il est plus interessant de representer la matrice des correlations de maniere regroupé (cluster) de facon à identifier les variables corrélé entre elle. On parle alors de partitionnement hierarchique agglomératif : Initialement, on a $n$ partitions, on reduit le nombre de classe 2 à deux en cherchant le minimum de distance : $\\min \\, \\{\\, d(a,b) : a \\in A,\\, b \\in B \\,\\}$, avec la distance euclidienne $d(a,b) = \\|a-b \\|_2 = \\sqrt{\\sum_i (a_i-b_i)^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlation matrix\n",
    "corr = df_explore.select_dtypes('float').corr()\n",
    "## visualisation and extract order list\n",
    "grid = sns.clustermap(corr, xticklabels=True)\n",
    "label_order = [xl.get_text() for xl in grid.ax_heatmap.get_xmajorticklabels()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Indiquer les blocs de corrélation. Essayez de voir surtout si les targets features appartiennent à un cluster specifique. Si oui, il est interessant de les noter, il peuvent permettre d'avoir une idée sur la reduction de variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i-bis) Mesure specifiques relation \"Feature Choix\" / \"Feature\"\n",
    "\n",
    "La matrice de corrélation nous à donnée des indicateurs de linéarité entre l'ensemble des variables, néamoins, dans le cas de relation non linéaire (polynome, parametrique, etc.), la corrélation est nulle alors qu'il existe bien une relation de correlation. Par exemple avec une fonction de puissance $f_a(x)=x^a$, en echelle log-log, on obtiendrait la relation linéaire $log(f_a(x)) = a.log(x)$ (principe de l'astuce du noyau - partie modelisation). Ainsi, il est preferable de regarder l'ensemble des relations entre les features \"target\" et les autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "for col in df_explore.select_dtypes('float'):\n",
    "    (fig, ax) = plt.subplots(figsize=(5, 5))\n",
    "    ax.set_xlabel('[target1, target2]'); ax.set_ylabel(col)\n",
    "    ## relation between each point folowing i target\n",
    "    ax.scatter(df_explore['target1'], df_explore[col], alpha = 0.3)\n",
    "    ax.scatter(df_explore['target2'], df_explore[col], alpha = 0.3)\n",
    "    plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Noter les artefacts, les corrélations apparente, si pour ces correlations, les données sont complete ou non. Si l'on observe un grand nombre de variables avec des données complete qui correle, il est plus interessant de s'orienter vers <span style=\"color:red\"> un probléme de régression</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Distribution des features par classes (Classification) : \n",
    "\n",
    "La mesure des distributions par classe de donnée permets de voir s'il existe un ecart entre deux distributions. Ici nous visualisons uniquement les distributions de densité pour l'ensemble des variables. On observes souvant 2 types de loi de probabilité (abusivement appelé distribution de propa) :\n",
    "\n",
    "- Loi Normale (continue) : $f(x) = c(\\sigma).e^{r(x,\\mu,\\sigma)^2}$ (monomodale, symétrique)\n",
    "- Loi de Poisson (discrète) : $ \\mathbb{P}(X=k)= \\frac {\\lambda^k}{k!}e^{-\\lambda} $\n",
    "\n",
    "Dans le cas de la loi normale, le mode (valeurs la plus fréquente dans un échantillon), la médiane (nombre qui divise en 2 parties equitable la population) et la moyenne (somme des valeurs de la variable divisée par le nombre d’individus) sont confondues.\n",
    "\n",
    "Lorsque le nombre de donnée est grand, on peut reduire la taille de l'echantillonnage, en calculant $N = \\frac{Z^2 \\sigma (1-\\sigma)}{\\epsilon^2}$, cette relation provient directement de l'inegalité de Bieneymé-Tchebychev (loi faible des grands nombres) et permet de definir l'intervalle de confiance d'une estimation de la moyenne. Aussi, nous pouvont representer une estimation du noyau (kde), qui consiste à construire des gaussiennes pour chacun des points de l'histogramme. (Aide principalement à la visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random reduction (Law of large numbers)\n",
    "reduced_df = df_explore.sample(1000, random_state=0)\n",
    "# drop all NAN\n",
    "reduced_df = df_explore.dropna()\n",
    "# verify if n > N\n",
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "for col in reduced_df.select_dtypes('float'):\n",
    "    print(col)\n",
    "    sns.displot(data=reduced_df[[col,'best_columns']], x=col, hue='classification_target',\n",
    "                kde=True, element=\"step\", stat=\"density\", common_norm=False)\n",
    "    plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Noter les variables où les distributions ne coincide pas (décalage). Si l'on observe un grand nombre de décalage, voire une séparation nettes entre beaucoup de distribution, il est plus interessant de s'orienter vers <span style=\"color:blue\"> un probléme de classification</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i)+(ii) Analyse bivariées\n",
    "Les deux analyses precedante peuvent etre résumé avec une visualisation des relations par paires (pairplot). Mais lorsqu'on a beaucoup de variable et d'echantillons, ils vaut mieux éviter ce type d'analyse lorsque le nombre de variable dépasse 10. Néamoins, on l'utilise lorsqu'on a la classification et la régression qui sont possible en meme temps, comme cela on peut comparer les relations entre elles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[interest_columns_list], hue=\"classification_target\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Dans certain cas, la séparation en 2 variables peut enlever la notion de linéarité, voire inverser la tendance (paradoxe de Simpson), si c'est le cas, privilégier la classification. Par contre, si la séparation en 2 variables laisse apparaitre des tendances linéaire alors qu'il n'y en avait pas initialement, la classification est encore à privilégier, néanmoins, envisager un sous échantillonage pour faire calculer une regression est possible. Dans les cas où les relations sont évidentes, la régression est à privilégier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Quantile proportion (specific variable)\n",
    "\n",
    "Lorsqu'on veut quantifier la proportion d'une variable specifiant dans quel categorie appartiennent les differents echantillons, nous pouvons faire une analyse quantile. La variables en x, est un quantile représenté sous forme de valeurs entiere (valeurs max : 4 pour quartile, 10 pour decile, 100 pour centile).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"int_variable\", hue='classification_target', data=df_explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Ce type de resultats nous informe s'il y a des seuils de valeurs, ou est-ce-que les valeurs sont concentré, etc. Ce type de graphes peut etre aussi utilisé pour une analyse temporelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-c. Data reduction visualization\n",
    "\n",
    "Lorsque le nombre des variables corrélées est grand (superieurs à 25), il existe des méthodes pour reduire la dimensionnalité du probleme. Cette méthodes consiste à projeter l'ensemble des points standardisé [(x-mean)/std] dans l'espace propre des covariances, on parle d'analyse en composante principale (PCA). Cette analyse décorrele les variables corrélé entres elles. À partir de la matrice de covariance, nous pouvons calculer les vecteurs de projection par le théorème de Caley-Hamilton (Polynome de matrice). On retrouve pour la premiere composante :\n",
    "\n",
    "<math> $$ \\mathbf{w}_{(1)}\n",
    " = \\arg\\max_{\\Vert \\mathbf{w} \\Vert = 1} \\,\\left\\{ \\sum_i (t_1)^2_{(i)} \\right\\}\n",
    " = \\arg\\max_{\\Vert \\mathbf{w} \\Vert = 1} \\,\\left\\{ \\sum_i \\left(\\mathbf{x}_{(i)} \\cdot \\mathbf{w} \\right)^2 \\right\\}\n",
    " = \\arg\\max \\left\\{ \\frac{\\mathbf{w}^\\mathsf{T} \\mathbf{X}^\\mathsf{T} \\mathbf{X w}}{\\mathbf{w}^\\mathsf{T} \\mathbf{w}} \\right\\} $$ </math> \n",
    "\n",
    "Lorsque le nombres d'echantillons est tres grand (>1M) mais car on a plusieurs serie temporelle non groupé (> 1k groupes). On peut reduire l'information par une décomposition en série de fourier et extraire uniquement les composante fondamentales harmonique. Cette méthodes peut reduire de beaucoup les données si le nombre de groupe est grand, ainsi, il est preferable avant ce type d'analyse de calculer la frequence d'echantillonage de Shannon-Nyquist $F_N = \\frac {f_e}{2} $, où $f_e$ est défini par \"\"\".\n",
    "\n",
    "Ici nous allons d'abords appliquer la transformations PCA, visualiser en 3D les echantillons projeté, on fini par representé la projetion des \"variables\" dans cette espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization (correlated_bloc without poverty)\n",
    "x = df_explore.loc[:, correlated_block].replace([np.inf, -np.inf], np.nan).dropna() # or centered fillna(0)\n",
    "x = skpp.StandardScaler().fit_transform(x)\n",
    "# PCA compression of continious variables (if unlinear : Manifold : IsoMap)\n",
    "pca = PCA(n_components=0.8)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "pca_df = principalComponents[:,:3]\n",
    "# dataframe pca\n",
    "explore_df_pca = pd.DataFrame(data = pca_df, columns = ['PC1', 'PC2', 'PC3'])\n",
    "# add class\n",
    "explore_df_pca['classification_target'] = df_explore['classification_target'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(explore_df_pca['PC1'], explore_df_pca['PC2'], explore_df_pca['PC3'], \n",
    "           c= explore_df_pca['classification_target'].cat.codes, \n",
    "           label = explore_df_pca['classification_target'].values)\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components in correlation circle\n",
    "components = pca.components_\n",
    "# dataframe\n",
    "pca_c_df = components[:3,:]\n",
    "# plot pca circle correlation\n",
    "(fig, ax) = plt.subplots(figsize=(8, 8))\n",
    "for i in range(0, pca.components_.shape[1]):\n",
    "    ax.arrow(0,0, pca.components_[0, i], pca.components_[1, i])\n",
    "    plt.text(pca.components_[0, i], pca.components_[1, i],correlated_block[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations :** Le déplacement en 3D permet de trouver les plans qui peuvent séparer les données. La projection des variables dans l'espace des composantes est une autres facons d'observer les clusters de correlation. Si la PCA sépare clairement les données, il est plus interessant de travailler dans cet espace pour la modelisation d'une classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-c. Data contingency\n",
    "\n",
    "La mesure de la contingence est realisé lorsqu'on a plusieurs variables qualitatives dans notre dataset. Elle permet d'estimer simplement en comptant, la dépendance entre deux caractere. On l'utilise aussi lorsqu'on a creer des varaibles qualitative, comme vide/non-vide, elle permet ainsi de mesurer l'effet de l'abscence d'information dans le dataset et voir si c'est homogene ou non. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contigency = pd.crosstab(df_explore['classification_target'], df_explore['categoryA'])\n",
    "print(df_explore['categoryA'].value_counts() , '\\n')\n",
    "print(df_explore['classification_target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(contigency, annot=True) # possible normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** Lorsque les proportions de quantité change, cela implique qu'il y a un effet à quantifier et à mettre en perspective par rapport à la proportion de donnée en question. (reformuler mieux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-d. Data Test\n",
    "\n",
    "À partir des données que l'on vient de collecter, nous avons uniqument fait des observations, maintenant il faut tester nos hypotheses à partir de test inférentiel. Une des méthodes est de supposer que les 2 distributions sont identiques, on parle d'hypothèse nulle H0. Ensuite, on utilise une statistique de test qui va rejeter ou non l'hypothese. La statistique que l'on utilise est celle de la comparaison des taux moyens de 2 distributions, on l'appele de Test de Student (ou t-test). La plupart des test statistique se base sur des loi normale, ce qui est possible car l'on peut faire l'approximation de l'echantillongage de n'importe quel loi de probabilité par une loi normale :\n",
    "\n",
    "$$ \\bar{X_n} = \\frac {X_1+ \\cdots + X_n }{n}; Z_n = \\frac {S_n - n \\mu  }{ \\sigma \\sqrt{n}};  \\lim _{n \\to \\infty } \\mathbb{P}(Z_n \\leq z) = \\Phi _{\\textit{N}(0,1)}(z); \\sigma \\to \\frac {\\sigma}{\\sqrt n}$$\n",
    "\n",
    "Le test de student permet de comparer la moyenne d'une loi normale à une valeur si la variance est inconnue, comparaison de deux moyennes issues de deux lois normales si leurs variances sont égales et inconnues, ou si leurs variances sont différentes et inconnues (Test t de Welch), tester sur les coefficients dans le cadre d'une régression linéaire. Test sur des échantillons appariés. \n",
    "\n",
    "**Remarque :** alternative avec la formule de Bayes.\n",
    "\n",
    "$$ P[T|D] = \\frac{P[D|T]P[T]}{P[D|T]P[T]+P[D|A]P[A]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance class (choice the min size)\n",
    "df_Test = df_explore[reducted_columns].dropna()\n",
    "A = df_Test[df_Test.classification_target == 'A'].sample(1000, random_state=0)\n",
    "B = df_Test[df_Test.classification_target == 'B'].sample(1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(df_1, df_2, col):\n",
    "    alpha = 0.02\n",
    "    stat, p = ttest_ind(df_1[col].dropna(), df_2[col].dropna())\n",
    "    if p < alpha:\n",
    "        return 'H0 Rejetée'\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns =  df_Test.select_dtypes('float').columns\n",
    "variables = []\n",
    "for col in feature_columns :\n",
    "    H0 = t_test(A,B,col)\n",
    "    if H0 != 0 :\n",
    "        variables += [col]\n",
    "    print(f'{col :-<50} {H0}')\n",
    "print(\"\\n Nombres d'hypothèses rejeté : \" + str(len(variables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations :** Identifier les variables où le test à été rejeté, cela implique qu'ils sont statistiquement significatifs (il y a une relation). En générale, cela confirme les hypotheses que l'on avait exploré auparavant. Il serait interessant aussi de classer l'ordre de la valeur p obtenu par le test de Student (à ajouter, code à corriger, surtout partit random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Demonstration Theoreme central limite :\n",
    "import numpy as np, pylab as plt\n",
    "n, N = int(1e5), 10\n",
    "\n",
    "X = N*np.random.rand(N,n)-(N/2)\n",
    "plt.hist(X[0,:],bins=100,density=1,histtype='step')\n",
    "plt.hist(X[1,:],bins=100,density=1,histtype='step')\n",
    "plt.hist(X[0,:]+X[1,:],bins=100,density=1,histtype='step')\n",
    "plt.hist(np.sum(X, axis=0),bins=100,density=1,histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-bonus. Image and NLP Data\n",
    "\n",
    "Image :\n",
    "\n",
    "- Average - size - distribution\n",
    "- Contrast - Variability - Eigen\n",
    "- Fourier - Convolution\n",
    "\n",
    "Language :\n",
    "\n",
    "- word frequency, sentence lenght\n",
    "- Ngram (time), link between words (bayes)\n",
    "- Intertopic distance map (LDA + Tokenize + bag of word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing\n",
    "\n",
    "Ici nous préparons les données pour l'entrainement d'un modele à partir de l'exploration precedante. On teste avec un modèle simple (regression linéaire / Decision Tree Classifier) pour trouver les modifications du jeu de données qui donne le meilleurs resultats du dataset de test.\n",
    "\n",
    "Suivant les variables d'interet qu'on a identifier et du type de modele qu'on a choisi, on commence par copier le dataset, ensuite, on enlève les variables de sortie inutiles dans le dataset, pour ne pas alimenter les modeles avec la solution. Si on a identifié une importance des variables avec un taux de vide élevé dans notre exploration, nous ne devons pas les enlevent directement ici (cas frequent où l'on a creer une variable de classification à partir de donnée continue).\n",
    "\n",
    "Pour cette étapes, nous n'utilisons pas un modele complexe, ainsi, on aura soit une grande variance, mais une un faible biais (regression), soit l'inverse (decision tree). L'esperance est défini par (dilemme biais-variance) :\n",
    "\n",
    "$$ \\mathrm{E}\\left[\\left(y - \\hat{f}(x)\\right)^2\\right] = \\mathrm{Biais}\\left[\\hat{f}(x)\\right]^2 + \\mathrm{Var}\\left[\\hat{f}(x)\\right] + \\sigma^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess = data.copy()\n",
    "\n",
    "## remove useless output solution (if necessary)\n",
    "features = variables.copy() # feature only & HO rejeted\n",
    "for e in ['f1','f2','f3'] : if e in features : features.remove(e)\n",
    "df_preprocess = df_preprocess[['indexation_variable','classification_target']+features] # if classification pb\n",
    "\n",
    "## remove empty colums (only if no relation !)\n",
    "select_columns = df_preprocess.columns[df_preprocess.isna().sum()/df_preprocess.shape[0] < 0.85]\n",
    "df_preprocess = df_preprocess[select_columns]\n",
    "\n",
    "## remove aberrant values (if we observe artefact previously)\n",
    "df_preprocess = df_preprocess[(df_preprocess.isin([0]).sum(axis=1)) > 1] # case when scaling generate absurdity\n",
    "\n",
    "## class numbers reduction to binary (if not relevant)\n",
    "binarization = (df_preprocess['classification_target'] == 'C1') | (df_preprocess['classification_target'] == 'C2')\n",
    "df_preprocess = df_preprocess[binarization]\n",
    "\n",
    "# show new properties\n",
    "print(df_preprocess.shape)\n",
    "print(df_preprocess['classification_target'].value_counts())\n",
    "df_preprocess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations :** Commenter la repartion des echantillons en cas de classification (balancé ou non), verfier s'il y a toujours des artefacts, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-a. Dataset training-test construction\n",
    "\n",
    "Contruire le dataset d'entrainement et de test, on ne touchera pas au dataset de test, mais on appliquera les meme procedure de preprocessing. Il faut que les données respecte une convention pour l'entrée \"x_feature^{exemple}\", cela est fait automatiquement sur sklearn. On affichera la répartion des label de sortie pour les dataset. Les données sont automatiquement randomisé, il est important que cela soit reproductible (random_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = skms.train_test_split(df_, test_size=0.2, random_state=0)\n",
    "# proportion of class repartition (try to distribute with the same proportion)\n",
    "trainset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-b. Data Encoding\n",
    "\n",
    "Convertir les labels en valeurs numériques pour les préparer au algorithme de machine learning de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df, target_name) :\n",
    "    # convert string to values\n",
    "    code = {'A':0,'B':1}\n",
    "    df.loc[:,target_name] = df[target_name].map(code)\n",
    "    \n",
    "    # with sklearn (for more complex cases)\n",
    "    le = skpp.LabelEncoder(); le.fit(df[target_name])\n",
    "    df.loc[:,target_name] = le.transform(df[target_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding(trainset, 'classification_target')\n",
    "encoding(testset, 'classification_target')\n",
    "trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "*Return here after basic test modelization (Trial and error method)*\n",
    "\n",
    "### 3-c. Data Transform\n",
    "\n",
    "Ici nous allons tester retro-activement les parametres de preprocessing sur des modeles simples de machine learning, pour eviter l'overfitting, l'underfitting, le sous-echantillonage, l'ajout de variable, etc. Cette partie est relative au type de donnée : tableau de données, image, et Langage (NLP).\n",
    "\n",
    "Dans le cas général, nous avons 3 outils à explorer ici, classé par ordre d'importance-utilité : (1) L'imputation de donnée lorsqu'on est en overfitting, (2) la creation de variable lorsqu'on est en sous-apprentissage, et (3), le suréchantillonage lorsqu'on a des classes non balancé (on peut faire l'inverse).\n",
    "\n",
    "(1) Contrairement au 2 suivantes, celle-ci est souvant indispensable pour l'apprentissage. Elle consiste, soit à suprimer les valeurs NaN, soit les estimer par des stratégies spécifique, les plus simple sont : 0, mean, median, etc., mais il y en a des plus sophistiqué, comme par exemple, l'approximation polynomiale (multivariate), ou encore KNN. L'algorithme KNN va calculer la valeur moyenne des valeurs des k plus proches voisins. Lien avec dimension VC ? \n",
    "\n",
    "$$ \\operatorname{RMSD}(\\hat{\\theta}) = \\sqrt{\\operatorname{MSE}(\\hat{\\theta})} = \\sqrt{\\operatorname{E}((\\hat{\\theta}-\\theta)^2)}. $$ \n",
    "\n",
    "(2) Pour la creation de variable, il existe plusieurs méthodes, la premiere est une méthodes consiste à ajouter les composantes principale dans les colonnes du jeu de donnée, on parle alors d'extraction de caracteristique, il y a aussi, une methodes par des AutoEncoder (deepLearning), la vectorisation des données, la combinaison de données (polynome), etc. Dans notres cas, nous pouvons creer de nouvelle classe automatique par la méthodes K-mean (+Elbow method). Pour cela, on place \"extremité ++\", puis on estime le nombre de cluster \"elbow method\". Ce type d'algorithme permet aussi de detecter les anomalie dans un jeu de donnée (outlier, voir aussi : isolationForest). L'algorithme cherche à minimiser le barycentre/inertie/variance tel que :\n",
    "\n",
    "$$ \\underset{\\mathbf{S}}{\\operatorname{arg\\,min}} \\sum_{i=1}^{k} \\sum_{\\mathbf x_j \\in S_i} \\left\\| \\mathbf x_j - \\boldsymbol\\mu_i \\right\\|^2  $$ \n",
    "\n",
    "(3) Le surechantillonnage est un cas frequent en apprentissage automatique, il survient lorsque la tailles des classes d'echantillons n'est pas le meme. Lorsque le jeu de donnée est grand (>100k), il n'est pas necessaire de creer plus de donnée, il suffit alors de reduire aléatoirement les données des classes en surplus (autre possibilité : cluster Kmean). Sinon, il existe des algorithmes qui vont creer artificiellement des données d'apprentissage, comme SMOTE. Si ce n'est pas concluant, utiliser une metrique F1 (voir à la suite). Il creer des données par (see article) :\n",
    "\n",
    "$$ \\delta(V_1,V_2) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    ### Feature imputation (overfitting)\n",
    "    # Simple (or intuitive)\n",
    "    '''df = df.dropna() ; df = df.fillna(0)'''\n",
    "    imputer = skimp.SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    df = simple_imputer.fit_transform(df)\n",
    "    # KNN\n",
    "    imputer = skimp.KNNImputer(n_neighbors=2)\n",
    "    df = imputer.fit_transform(df)\n",
    "    \n",
    "    ### feature-label engineering (underfitting)\n",
    "    # feature\n",
    "    df = pd.merge(df, explore_df_pca, how='inner', on='merge_column')\n",
    "    # label\n",
    "    model = KMeans(n_clusters= 3, init='k-means++').fit(df) ; print(model.inertia_)\n",
    "    df['cluster'] = model.labels_\n",
    "    \"\"\"df[['nb_feature_center']] = model.cluster_centers_[model.labels_] # fit transform for distance \"\"\"\n",
    "    # anomaly\n",
    "    df = df[IsolationForest(contamination=0.01).fit(df)]\n",
    "    \n",
    "    ### Oversampling (undersampling)\n",
    "    # drop\n",
    "    df_c1 = df[df.classification_target == 'A'] #lowest\n",
    "    df_c2 = df[df.classification_target == 'B'].sample(df_c1.shape[0], random_state=0)\n",
    "    df = pd.concat([df_c1, df_c2])\n",
    "    \"\"\"under = RandomUnderSampler(sampling_strategy=0.5)\"\"\"\n",
    "    # Smote\n",
    "    over = SMOTE(sampling_strategy=0.1)\n",
    "    df = over.fit_resample(df)\n",
    "    \n",
    "    ### input / target\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_image(df):\n",
    "    # image morphology (artefact delete)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    ### feature engineering (underfitting)\n",
    "    df['new'] = df[] # ex : polynomial feature \"optimize/extraction\", binarizer, kernel log/log. Oversampling :\n",
    "    # unsupervised_info = Kmean clustering class (+Elbow method for good seeder), PCA axis, Isolation Forest (Anomaly)\n",
    "    ### feature imputation (overfitting) - SimpleImputer(strategy='')\n",
    "    df = df.dropna() # drop-fill-NA, KNNImputer, image morphology (artefact delete). feature selection :\n",
    "    # selector = VarianceThreshold() # constante columns, other : selectKbest (Khi-2)\n",
    "    #feature select : SelectFromModel (if parameter model, KNN not included)\n",
    "    # input / target\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le texte..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_language(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarques :** Ici nous avons vu les opérations de preprocessing qu'on applique pour l'ensemble du dataset et le modifiant definitivement. Néamoins, il y a des opérations de transformation que l'on placera dans la pipeline d'entrainement, ce qui permetra de tester un plus large panel de parametre de transformation. C'est le cas de la normalisation, le changement d'echelle (semilog), l'interaction de caracteristique (ex : augmentation polynomiale) et/ou la selection de feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data modelization\n",
    "\n",
    "Trouver un modele ayant dont la \"métrique\"/\"fonction de cout\" suit un seuil predifini. Cela peut etre le score F1, ou la précision pour un probleme de classification, ou le RMS, le score R2 pour des problemes de régression. On teste du modele le plus simple juTester du plus simple au plus complexe et revenir au pré-traitement si necessaire. Quelques soit le modele, on distingue 3 composantes : (1) les parametres du modele (ou leur absence), (2) la fonction de couts (estimation), et (3), l'algorithme de minimisation. Nous allons voir ces 3 points, pour les 2 modeles les plus simple, la regression linéaire, et le modèle logit (aussi appelé, regression logisitique).\n",
    "\n",
    "La regression linéaire est un modele qui cherche à établir une relation linéaire entre une variable, dite expliquée, et une ou plusieurs variables, dites explicatives. Ce modèle suit les hypothese de Gauss-Markov : non colinéarité des variables explicatives, l'indépendance des erreurs et la normalité des termes d'erreurs. La normalité du terme d'erreur implique que pour chaque points en absisse, l'erreur suit une loi normale centrée de variance $ \\sigma^2$. Ce modele s'ecrit $Y = X \\beta + \\epsilon $. On distingue plusieurs estimateurs pour le calcul de la fonction de cout, la méthodes des moindres carré ou Khi-2, qui valcul l'erreur quadratique pour chaque points, ou encore la methode R2, un equivalent adaptatif (1-RMS). Enfin, pour la minimisation de la fonction de cout, on a soit la methodes normales qui determinie directement par inversion de matrice les coefficient optimaux, mais demande d'inverse une matrice de taille proportionnel au carré des points en entrée, soit la méthodes de descente de gradient, où l'on reduit pas à pas, la quantité d'erreur :\n",
    "\n",
    "$$ Y_i = $$\n",
    "\n",
    "La regression logisitique, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocessing(trainset)\n",
    "X_test, y_test = preprocessing(testset)\n",
    "# if not dataframe (numpy array), add shape if (N,) to be (1,N) : reshape or [None]\n",
    "# warning of numpy broadcoasting if numpy : (1,3) + (3,1) = (3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-a. Basic model testing\n",
    "\n",
    "Autres modele à tester et à decrire ici, Lasso et DecisionTree. Notion de Pipeline sur Sklearn (Transformer+Estimator chain) : cross validation, select-kbest et learning curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalutation(model) :\n",
    "    # fit\n",
    "    model.fit(X_train, y_train) # step 3,4 directly included\n",
    "    # first evaluation\n",
    "    #model.score(X,y) #R2 automatics metric if regression\n",
    "    # predict\n",
    "    ypred = model.predict(X_test)\n",
    "    # evaluation\n",
    "    print(confusion_matrix(y_test, ypred)) #(sum=support) nb_success A, nb_error A ## nb_succes B, nb_error B\n",
    "    print(classification_report(y_test, ypred))\n",
    "    # learning and validation curve (with cross validation : Kfold per default)\n",
    "    N, train_score, val_score = learning_curve(model, X_train, y_train,cv=4, scoring='f1',train_sizes=np.linspace(0.1, 1, 10))\n",
    "    # learning curve F1 = precision/recall (see overfit, underfit)\n",
    "    plt.plot(N, train_score.mean(axis=1), label='train score')\n",
    "    plt.plot(N, val_score.mean(axis=1), label='validation score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele sur des tableaux de donnée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "test_model = DecisionTreeClassifier(random_state=0) # first\n",
    "#test_model = RandomForestClassifier(random_state=0) # regularized model (basic ensemble)\n",
    "#test_model = make_pipeline(PolynomialFeatures(2), SelectKBest(f_classif, k=10), RandomForestClassifier(random_state=0)) # here contain data transform included in model\n",
    "# evaluation\n",
    "evalutation(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele sur des images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele sur du texte (classifieur naif bayesien) :\n",
    "\n",
    "$$ P[T|D] = \\frac{P[(D,A)|T]P[T]}{Z} = \\frac{P[D|T].P[A|(T,D)].P[T]}{Z} = \\frac{P[D|T].P[A|T].P[T]}{Z} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of feature importance (for selection)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_model.feature_importances_, index=X_train.columns).plot.bar(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*When the precision, recall and f1-score is > 50% (convergence of Law of large numbers), the dataset it's good*\n",
    "\n",
    "### 4-b. Ensemble learning\n",
    "\n",
    "Ensemble learning = Law of large numbers + Competence + Diversity\n",
    "\n",
    "ici decrire Polynomiale feature en plus. ensuite, SVM, AdaBoost, et GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_pipeline(PolynomialFeatures(2, include_bias=False), SelectKBest(f_classif, k=10))\n",
    "#preprocessor = make_columns_selector((categorical_pipeline, ['cat1','...','catn']),(numerical_pipeline, ['v1','...','vn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification model (basic = logistic regression)\n",
    "RandomForest = make_pipeline(preprocessor, RandomForestClassifier(random_state=0))\n",
    "AdaBoost = make_pipeline(preprocessor, AdaBoostClassifier(random_state=0))\n",
    "SVM = make_pipeline(preprocessor, StandardScaler(), SVC(random_state=0))\n",
    "KNN = make_pipeline(preprocessor, StandardScaler(), KNeighborsClassifier())\n",
    "# regression model (basic = linear regression)\n",
    "\"\"\"\n",
    "LinearRegression\n",
    "etc.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_models = {'RandomForest': RandomForest, # bagging : all in overfitting grouping result\n",
    "                  'AdaBoost' : AdaBoost, # boosting : all in underfitting grouping result\n",
    "                  'SVM': SVM,\n",
    "                  'KNN': KNN\n",
    "                 }\n",
    "\"\"\" Note : Stacking it's for predictor of predictor, VotingClassifier method for bagging/boosting without diversity \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in dict_of_models.items():\n",
    "    print(name)\n",
    "    evaluation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model choisi :** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-c. Hyperparameter search\n",
    "\n",
    "decrire la methode de grille de recherche.\n",
    "\n",
    "Theoreme d'optimisation de dualité ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chosen model \n",
    "\"\"\"\"\n",
    "# 1st for grid\n",
    "hyper_params = {'svc__gamma':[1e-3, 1e-4],\n",
    "                'svc__C':[1, 10, 100, 1000, 3000]}\n",
    "\"\"\"\n",
    "hyper_params = {'svc__gamma':[1e-3, 1e-4, 0.0005],\n",
    "                'svc__C':[1, 10, 100, 1000, 3000], \n",
    "               'pipeline__polynomialfeatures__degree':[2, 3],\n",
    "               'pipeline__selectkbest__k': range(45, 60)}\n",
    "# last it's not adapted for grid (too many parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(SVM, hyper_params, scoring='recall', cv=4) # first for some parameter\n",
    "grid = RandomizedSearchCV(SVM, hyper_params, scoring='recall', cv=4, n_iter=40) # cv = crossvalidation (cutting in 4 validation set of train set here)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-d. Threshold search\n",
    "\n",
    "optimisation finale du modele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(y_test, grid.best_estimator_.decision_function(X_test))\n",
    "plt.plot(threshold, precision[:-1], label='precision')\n",
    "plt.plot(threshold, recall[:-1], label='recall')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-e. Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_final(model, X, threshold=0):\n",
    "    return model.decision_function(X) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_final(grid.best_estimator_, X_test, threshold=-1)\n",
    "# score for classification model\n",
    "f1_score(y_test, y_pred)\n",
    "recall_score(y_test, y_pred)\n",
    "# score for regression model\n",
    "\"\"\"\n",
    "mean_absolute_error(y_test, y_pred) # linear problem\n",
    "mean_squared_error(y_test, y_pred) # exponential problem\n",
    "median_absolute_error(y_test, y_pred) # if some outlier = robust (warning : high risk)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-Bonus. Modele pour Image et NLP.\n",
    "\n",
    "Image : Gradient de Flot, Reseau de neurone, convolution, Transfert Learning, Encoder-Decoder.\n",
    "\n",
    "NLP : Baysesian, RNN (LSTM) et Attention.\n",
    "\n",
    "Qiskit : approche au ML quantique : neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Chosen model, link between other report, good variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-a. Synthetic visualization\n",
    "\n",
    "*Relative to dataset :* geographic overview, pairplot, reduction_observation, feature_selection, evaluation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(12,12))\n",
    "#gs = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "#gridkw = dict(height_ratios=[5, 1])\n",
    "#fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw=gridkw)\n",
    "\n",
    "#data[5][data[5].CODGEO.str[:2] != \"97\"].plot(ax=ax, categorical=True, column = \"Results\", legend=True)\n",
    "#sns.clustermap(data=corr, xticklabels=True) #, ax=gs[0,0])\n",
    "#pd.DataFrame(feature_weight, index=X_train.columns).plot.bar(figsize=(12, 8))\n",
    "\"\"\"\n",
    "plt.plot(threshold, precision[:-1], label='precision')\n",
    "plt.plot(threshold, recall[:-1], label='recall')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-b. Discussion\n",
    "\n",
    "Possible amelioration (if more time)\n",
    "\n",
    "---\n",
    "\n",
    "### Supplementary\n",
    "\n",
    "$$ t \\subset \\bigcup \\mathbb{R} \\cap \\alpha \\in \\cdots \\lim_{n \\to \\infty} \\frac{b-a}{n} \\sum_{k=1}^{n} \\sqrt[n]{\\left \\| a + k \\frac{b-a}{n} \\right \\|} \\mapsto \\int_{a}^{b} \\sqrt[n]{\\| t \\|} \\partial x \\Leftrightarrow \\begin{matrix}\n",
    "a & b & c\\\\ \n",
    "d & e & f\\\\ \n",
    "g & h & i\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store basic variable after closing session (for seeing curve faster)\n",
    "plot_var = {'df': 0 }\n",
    "%store plot_var"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

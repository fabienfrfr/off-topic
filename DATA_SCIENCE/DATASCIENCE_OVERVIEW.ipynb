{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour lancer Jupyter Notebook, il faut click-droit/Ouvrir un Terminal, puis lancer la commande 'jupyter notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"11 travaux de l'an 2005\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{5+6} travaux de l'an {2000+5}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petit Guide de la Data\n",
    "---\n",
    "\n",
    "Nous allons voir ici les bases de connaissance pour être un travailleurs de la Data. Ce petit guide est destiné aussi bien au scientifique voulant utiliser les outils informatiques lié à la data, que les developpeurs polyvalent (Back-end : Python, PHP. Front-end : HTML, CSS).\n",
    "\n",
    "Depuis l'apparition du stockage numérique, la quantité de donnée et d'information ne fait qu'exploser et double tout les ans. Il faut donc savoir comment analyser le monde pour les entreprises sans etre perdu par l'information (outlier, redondance). C'est pour cette raison que les metiers de la data sont devenu necessaire. Pour les entreprises, la données reste plus important que les modeles pour l'instant, mais avec la montée en puissance de l'IA, les deux vont etre necessaire.\n",
    "\n",
    "https://youtu.be/f3U91xgn5BQ\n",
    "\n",
    "Leur rôle est de produire des méthodes de tri et d'analyse de données afin d'en extraire des informations. Dans le milieux de l'entreprise, voici 3 exemples de missions :\n",
    "\n",
    "* Mesurer l'activité d'une audiance sur les réseaux sociaux, savoir l'activité d'un client en dehors de la marque (ciblage)\n",
    "* Aider un service marketing à envoyer un mail au 10% des utilisateurs les plus réguliers, et mesurer l'impact de l'automatisation.\n",
    "* Prédire la tendance d'un marché, détecter des anomalies, etc.\n",
    "\n",
    "\n",
    "Nous distinguons plusieurs outils à maitriser, nous en détaillerons 6. \n",
    "Deux pour l'analyse avec Le SQL et La bibliothèque Pandas.\n",
    "Deux ensuite, pour la collecte et la visualisation, avec Web Dumping et Gephi.\n",
    "Deux derniers pour le machine learning, avec PyTorch et Sklearn.\n",
    "\n",
    "On distingues 3 grandes branches : Le \"Data Analyst\" qui travaille sur des données déjà existantes. Le \"Data Scientist\" qui va creer des données à partir d'un autre outils (exploration), comme le web, du texte, etc et qui decrit des tendances (insight). Et, le \"Machine Learning Engineer\", qui va essayer de prédire des resultats à partir de donnée nouvelle sur des modele preentrainé.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Le SQL (Data Analyst)\n",
    "***\n",
    "\n",
    "SQL est un langage informatique normalisé servant à exploiter des bases de données relationnelles. Pour tester SQL, il faut donc une base de donnée, un serveur et un client pour interagir et acceder au données. Pour cela, il existe ce que l'on appele des systèmes de gestion de base de données (SGBD), comme MySQL. Dans notre cas, nous utiliserons PostgreSQL, car il est libre, adapté à Python et surtout Jupyter ! (Attention, pas compatible entre elle)\n",
    "\n",
    "Pour l'installation (https://medium.com/analytics-vidhya/postgresql-integration-with-jupyter-notebook-deb97579a38d) :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# intallation des programme client-serveur\n",
    "sudo apt install postgresql\n",
    "sudo apt install postgresql-server-dev-all\n",
    "\n",
    "# installation des modules necessaires\n",
    "pip3 install ipython-sql\n",
    "pip3 install sqlalchemy\n",
    "pip3 install psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir une base de donnée d'entrainement, il y a : https://www.postgresqltutorial.com/postgresql-sample-database/. On obtient ainsi le fichier **dvdrental.tar**. Généralement, les entreprises disposes de base de donnée en internes. Une base de données générale peut etre obtenu via le dumps de wikipédia : https://dumps.wikimedia.org/frwiki/latest/\n",
    "\n",
    "L'installation ajoute l'utilisateur postgres à la liste des utilisateurs. Utilisez finger si vous souhaitez plus informations (bash : finger postgres). Et c'est tout, il n'y a pas plus de chose à installer. Dans une entreprise, on se connecte à un serveur qui herberge la base de donnée, mais dans notre cas, nous utilisons une base de donnée qui est dans l'environement local (notre ordinateur, Hote : 127.0.0.1). Pour ajouter et tester une base, faire à la suite sur l'invite de commande (possible aussi sur pgAdmin) :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# step 1 :\n",
    "sudo -i -u postgres\n",
    "\n",
    "# step 2 :\n",
    "psql\n",
    "\n",
    "# step 3 :\n",
    "CREATE DATABASE dvdrental;\n",
    "\n",
    "# step 4 :\n",
    "ALTER USER postgres PASSWORD 'myPassword';\n",
    "\n",
    "# step 5 :\n",
    "exit\n",
    "\n",
    "# step 6 :\n",
    "pg_restore --dbname=dvdrental --verbose /home/fabien/Documents/dvdrental.tar\n",
    "\n",
    "# step 7 : \n",
    "psql\n",
    "\n",
    "# step 8 :\n",
    "\\c dvdrental\n",
    "\n",
    "# step 9 : \n",
    "select count(*) from film;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite pour se connecter à lancer une commande sql, on utilise \"%\". Pour se connecter à la base (+mapping interfacage python/sql via sqlalchemy), suivre il suffit de faire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "%sql postgresql://postgres:myPassword@localhost/dvdrental\n",
    "engine = create_engine('postgresql://postgres:myPassword@localhost/dvdrental')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code peut etre tester ensuite en sql et aussi via le moteur par les instructions suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@localhost/dvdrental\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>inventory_id</th>\n",
       "        <th>film_id</th>\n",
       "        <th>store_id</th>\n",
       "        <th>last_update</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3</td>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 1, 1, datetime.datetime(2006, 2, 15, 10, 9, 17)),\n",
       " (2, 1, 1, datetime.datetime(2006, 2, 15, 10, 9, 17)),\n",
       " (3, 1, 1, datetime.datetime(2006, 2, 15, 10, 9, 17))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    inventory\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inventory_id</th>\n",
       "      <th>film_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inventory_id  film_id  store_id         last_update\n",
       "0             1        1         1 2006-02-15 10:09:17\n",
       "1             2        1         1 2006-02-15 10:09:17\n",
       "2             3        1         1 2006-02-15 10:09:17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "inventory_dataframe = pd.read_sql('SELECT * FROM inventory LIMIT 3', engine)\n",
    "inventory_dataframe.head() #pratique pour afficher les premieres lignes de donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelque instruction pour comprendre SQL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@localhost/dvdrental\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>inventory_id</th>\n",
       "        <th>film_id</th>\n",
       "        <th>store_id</th>\n",
       "        <th>last_update</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3</td>\n",
       "        <td>1</td>\n",
       "        <td>1</td>\n",
       "        <td>2006-02-15 10:09:17</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 1, 1, datetime.datetime(2006, 2, 15, 10, 9, 17)),\n",
       " (2, 1, 1, datetime.datetime(2006, 2, 15, 10, 9, 17)),\n",
       " (3, 1, 1, datetime.datetime(2006, 2, 15, 10, 9, 17))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# afficher les 3 premiers element de la base dvdrental de la base inventory\n",
    "%sql SELECT * FROM inventory LIMIT 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour s'attaquer à de plus grande base de donnée et autre que du texte, sql n'est pas le plus adapter pour le traitement. Pour cela il existe d'autre outils, ou les données sont déjà prestructurée dans plusieur serveur de fichier (HDFS) pour traiter l'information via un patron d'architechture (MapReduce), comme Hadoop. Certain sont encore plus adapté pour le machine learning, comme Spark. Pour les utiliser, il est necessaire de passer par des commandes unix (il suffit de mettre un \"!\" en debut de ligne). Néamoins nous ne verrons ici que les commande de HDFS et MapReduce, pour plus d'information, aller sur https://datascience-enthusiast.com/Hadoop/HDFS_Jupyter.html et https://medium.com/geekculture/mapreduce-with-python-5d12a772d5b3. Telechargment sur Kaggle de la base de donnée des commentaires d'hotels au format csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASCIENCE_OVERVIEW.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "import hdfs\n",
    "client = hdfs.InsecureClient(\"http://0.0.0.0:50070\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pandas (Data Analyst)\n",
    "***\n",
    "\n",
    "Une fois que l'on a collecter les données, formats texte pour l'analyse. Tableurs avant pour vue d'ensemble. (pivot-table equivalent)\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Analyse_des_donn%C3%A9es\n",
    "\n",
    "Quelque outils statistique de base, mais plus de detail dans la partie Learning.\n",
    "\n",
    "https://www.youtube.com/watch?v=eMOA1pPVUc4\n",
    "\n",
    "ajouter la notion de correlation !! + aggregate ! + ratio https://www.youtube.com/watch?v=SaTcOyjfrNc\n",
    "\n",
    "avec ca, polyfit (test kolmogorov, correspondance entre deux distribution), pour regression linéaire simple (+kernel trick si necessaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Web Scraping (Data Scientist)\n",
    "***\n",
    "\n",
    "Lorsqu'on n'a pas de donnée disponible (ce qui est souvent le cas), et que l'on veut exploiter les données d'un reseaux sociale ouvert. On doit générer des données.\n",
    "\n",
    "https://www.youtube.com/watch?v=Wvc2ZqdIPpk  \n",
    "https://www.youtube.com/watch?v=Ewgy-G9cmbg  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gephi (Data Scientist)\n",
    "***\n",
    "\n",
    "Cartographier nos données, exploration (plus pratique que Networkx)\n",
    "\n",
    "Caracteriser tendance ? (insight)\n",
    "\n",
    "https://www.youtube.com/watch?v=gcfAT8aMxuQ  \n",
    "https://www.youtube.com/watch?v=HJ4Hcq3YX4k  \n",
    "https://www.youtube.com/watch?v=PouhDHfssYA  \n",
    "\n",
    "These de romain aussi ! pour intuition centralité d'un graphe.\n",
    "\n",
    "Graphe cluster, Clique, Chemin (hamiltonien, induit, eulerian, cover) : des outils pratiques d'analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SkLearn (DATA Learning)\n",
    "***\n",
    "\n",
    "Augmentation des données par transformation, similarité des données (metric dice), partitionnement.\n",
    "\n",
    "se baser sur les données precedante ? L'idée est d'avoir un large panel d'outils pratique à savoir utiliser concretement ! Dans notre cas, savoir si un client a aimé un produit à partir d'un commentaire sur Amazon.\n",
    "\n",
    "mettre avant pytorch ! Json en bonus !\n",
    "\n",
    "https://www.youtube.com/watch?v=M9Itm95JzL0\n",
    "\n",
    "PCA, AFC, ACM et Canonique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PyTorch (DATA Learning)\n",
    "***\n",
    "\n",
    "Permet d'utiliser des reseau de neurones. prédire classification et régression. Uniquement NLP ici, pas besoin d'image. car l'idée est de recuperer information du web.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html  \n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html  \n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html  \n",
    "https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html  \n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
